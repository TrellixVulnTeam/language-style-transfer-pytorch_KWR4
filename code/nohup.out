/data/anaconda3/envs/madhu/lib/python3.8/site-packages/torch/nn/modules/rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
------------------------------------------------
{   'batch_size': 64,
    'beam': 1,
    'dev': '../data/yelp/sentiment.dev',
    'dim_emb': 100,
    'dim_y': 200,
    'dim_z': 500,
    'dropout_keep_prob': 0.5,
    'embedding': '',
    'filter_sizes': '1,2,3,4,5',
    'gamma_decay': 1,
    'gamma_init': 0.1,
    'gamma_min': 0.1,
    'learning_rate': 0.0005,
    'load_model': False,
    'max_epochs': 20,
    'max_seq_length': 20,
    'max_train_size': -1,
    'model': '../tmp/model',
    'n_filters': 128,
    'n_layers': 1,
    'online_testing': False,
    'output': '../tmp/sentiment.dev',
    'rho': 1,
    'steps_per_checkpoint': 1000,
    'test': '',
    'train': '../data/yelp/sentimentshort.train',
    'vocab': '../tmp/yelp.vocab'}
------------------------------------------------
#sents of training file 0: 300
#sents of training file 1: 300
vocab size:  180
Epoch:  0
Loss:  1890.2227978701708
---------

Epoch:  1
Loss:  1616.8788403962849
---------

Epoch:  2
Loss:  1452.8467596169353
---------

Epoch:  3
Loss:  1257.279254168323
---------

Epoch:  4
Loss:  1146.9512645851562
---------

Epoch:  5
Loss:  1055.4017538419428
---------

Epoch:  6
Loss:  1017.4999520722764
---------

Epoch:  7
Loss:  994.948389625433
---------

Epoch:  8
Loss:  950.7604835518404
---------

Epoch:  9
Loss:  924.2533478569201
---------

Epoch:  10
Loss:  903.5968224243949
---------

Epoch:  11
Loss:  897.6734095785934
---------

Epoch:  12
Loss:  876.822813667337
---------

Epoch:  13
Loss:  859.0193828739382
---------

Epoch:  14
Loss:  823.5642164054517
---------

Epoch:  15
Loss:  840.4973202965054
---------

Epoch:  16
Loss:  826.6514629443777
---------

Epoch:  17
Loss:  811.4248592306261
---------

Epoch:  18
Loss:  800.0209093088631
---------

Epoch:  19
Loss:  794.922827288927
---------

Epoch:  20
Loss:  780.2679856036607
---------

Epoch:  21
Loss:  776.1625882985683
---------

Epoch:  22
Loss:  761.262684924029
---------

Epoch:  23
Loss:  786.447139409348
---------

Epoch:  24
Loss:  752.8493004437623
---------

Epoch:  25
Loss:  744.060403571053
---------

Epoch:  26
Loss:  764.8148258701785
---------

Epoch:  27
Loss:  741.8432548208841
---------

Epoch:  28
Loss:  750.7372373883687
---------

Epoch:  29
Loss:  765.1511482249832
---------

Epoch:  30
Loss:  740.7136521659055
---------

Epoch:  31
Loss:  741.9426336070702
---------

Epoch:  32
Loss:  739.20911992324
---------

Epoch:  33
Loss:  730.4790493738956
---------

Epoch:  34
Loss:  724.0977540802753
---------

Epoch:  35
Loss:  704.7638347754112
---------

Epoch:  36
Loss:  717.8702276748908
---------

Epoch:  37
Loss:  709.0482746878531
---------

Epoch:  38
Loss:  712.2586695597721
---------

Epoch:  39
Loss:  701.5614041046342
---------

Epoch:  40
Loss:  716.3697407002271
---------

Epoch:  41
Loss:  703.2643229102624
---------

Epoch:  42
Loss:  683.3911019228316
---------

Epoch:  43
Loss:  682.0135286409485
---------

Epoch:  44
Loss:  685.3878524704294
---------

Epoch:  45
Loss:  711.1163829816569
---------

Epoch:  46
Loss:  696.3344925532847
---------

Epoch:  47
Loss:  695.7268298847773
---------

Epoch:  48
Loss:  698.3038122812326
---------

Epoch:  49
Loss:  692.5525319023445
---------

Epoch:  50
Loss:  689.5941721146214
---------

Epoch:  51
Loss:  691.3798249295342
---------

Epoch:  52
Loss:  678.3941792478491
---------

Epoch:  53
Loss:  696.1693305420496
---------

Epoch:  54
Loss:  686.6175916065956
---------

Epoch:  55
Loss:  697.0728487386806
---------

Epoch:  56
Loss:  698.979190996453
---------

Epoch:  57
Loss:  682.0755294218461
---------

Epoch:  58
Loss:  663.0078379413875
---------

Epoch:  59
Loss:  667.9659982621304
---------

Epoch:  60
Loss:  650.6656215716922
---------

Epoch:  61
Loss:  672.7715101771591
---------

Epoch:  62
Loss:  672.3010325279281
---------

Epoch:  63
Loss:  665.8872539661452
---------

Epoch:  64
Loss:  679.7220397485773
---------

Epoch:  65
Loss:  644.0623663593999
---------

Epoch:  66
Loss:  646.8815371375383
---------

Epoch:  67
Loss:  659.2105308503894
---------

Epoch:  68
Loss:  634.0859178499426
---------

Epoch:  69
Loss:  635.2627117374149
---------

Epoch:  70
Loss:  658.8971119177065
---------

Epoch:  71
Loss:  652.8697797083012
---------

Epoch:  72
Loss:  663.0367074005189
---------

Epoch:  73
Loss:  642.4798488618108
---------

Epoch:  74
Loss:  643.7520811811034
---------

Epoch:  75
Loss:  641.9446694841316
---------

Epoch:  76
Loss:  631.5241270201257
---------

Epoch:  77
Loss:  617.586069940327
---------

Epoch:  78
Loss:  613.9900992558553
---------

Epoch:  79
Loss:  625.0106059464198
---------

Epoch:  80
Loss:  627.1822622296866
---------

Epoch:  81
Loss:  657.5649380624512
---------

Epoch:  82
Loss:  650.6633948077797
---------

Epoch:  83
Loss:  640.7496149141127
---------

Epoch:  84
Loss:  618.9721221306093
---------

Epoch:  85
Loss:  638.6758561231277
---------

Epoch:  86
Loss:  623.5979387835268
---------

Epoch:  87
Loss:  621.6082677567833
---------

Epoch:  88
Loss:  629.702712761788
---------

Epoch:  89
Loss:  618.7073050938536
---------

Epoch:  90
Loss:  630.6065817901359
---------

Epoch:  91
Loss:  622.5719535117621
---------

Epoch:  92
Loss:  605.2309066935481
---------

Epoch:  93
Loss:  622.0303569831254
---------

Epoch:  94
Loss:  609.5771391717797
---------

Epoch:  95
Loss:  631.3069690683386
---------

Epoch:  96
Loss:  624.9582007390095
---------

Epoch:  97
Loss:  608.319143536414
---------

Epoch:  98
Loss:  595.4826310688468
---------

Epoch:  99
Loss:  643.5210388712248
---------

